% !TEX root = ../thesis.tex
\chapter{Computational Background}\label{chap:nlp}

\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

In this chapter I introduce related work in Natural Language Processing (NLP) and theoretical background on the machine learning methods that I use throughout this dissertation.

% \subsection{Abusive Language Detection}
% % \zw{Update this}
% % Abusive language detection is a growing field of inquiry. Much off the early work focused on cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with little focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified has surfaced as an independent task \citep{Waseem:2016,Waseem-Hovy:2016,Davidson:2017,Tulkens:2015,Agarwal:2016,Silva:2016,Park:2017,Samghabadi:2017}.
%
% A large part of the previous work on hate speech detection has primarily touched upon surface level analysis of abusive language, leaving much room for work to be done. A large effort has been expended in attempting to define annotation schemes. \cite{Waseem-Hovy:2016} proposed guidelines derived from gender studies \citep{McIntosh:1988} where a document is labeled as hate speech, if it fails any point in the guidelines.
%
% \cite{Waseem-Hovy:2016} released a data set for sexist and racist speech on social media which is annotated using their guidelines. In their paper, they investigate the impact of several features on detecting racism and sexism. They find that characters are more discriminative for hate speech detection in line with the findings of \cite{Mehdad:2016}. In addition, \cite{Waseem-Hovy:2016} find that information about a users gender can slightly improve classification performance, however they also find that adding location information slightly harms a classifiers performance. In addition, they find that information on length negatively impacts a classifiers performance.
%
% \cite{Ross:2016} investigate annotator agreement for anti-refugee sentiment. They instruct their annotators to follow the Twitter's guidelines for hateful content. They find that on a data set of 541 tweets, they achieve a very poor inter-annotator agreement, suggesting that it is necessary for clear and concise guidelines for annotation of abusive language.
%
% Building on the work of \cite{Waseem-Hovy:2016} and \cite{Ross:2016}, \cite{Waseem:2016} consider the impact of annotators' knowledge of hate speech for building models for hate speech detection; they find that employing feminist annotators for labeling data sets allows for more consistent annotations and models as compared to annotators that are not screened for political opinion. \cite{Waseem:2016} consider the application of features from sarcasm detection, using Author Historical Salient Terms (AHST) proposed by \cite{Bamman:2015}. The feature is generated by computing TF-IDF scores for each user and selecting the 100 highest weighted terms. If a term then occurs both in the document being analyzed and in the AHST. \cite{Waseem:2016} find that AHST performs extremely poorly, suggesting that hate speech may generally be a one off event, rather than a continuous stream of abuse. It is our contention that another reason AHST might not work is due to the data set employed being highly imbalanced. 
%
% \cite{Davidson:2017} seek to break down the task of hate speech detection into offensive language and hate speech and obtain labels for a Twitter data set using crowd sourced labor on CrowdFlower.
%
% More recently \cite{Badjatiya:2017} trained a deep convolutional neural network (CNN) on the data set annotated by \cite{Waseem-Hovy:2016}. By using a CNN on the data set \cite{Badjatiya:2017} obtain a significant improvement on Waseem and Hovy's (2016) scores improving the F1 score from  $73.89$ to $93.00$. Given the large increase in scores it is prudent to consider any potential errors. The data set \cite{Badjatiya:2017} employ, is highly imbalanced with the positive classes occupying a small minority of the labeled data, there is a risk that their model performs extremely well on the negative class but does not perform well on the positive classes. However, no error analysis is provided in the paper.
%
% In continuation of the results obtained by \cite{Badjatiya:2017}, \cite{Park:2017} compare using a two-step logistic regression classification, and a single step CNN approach to detecting hate speech. In the single step CNN, the specific form of hate speech is directly predicted, while in the two-step classification scenario, first a classifier is trained to identify whether a document contains abuse followed by predicting the specific type of abuse it is.
%
% A different approach is attempted by \cite{Waseem:2018}, in which they seek to combine three different datasets for abusive language detection using multi-task learning. With a hypothesis that abuse will differ between geographic and cultural locales, they seek to employ disjoint datasets and train two models, one for each data set that share parameters. We will seek to extend this work to employ more data sets of abusive language as well as related tasks, such as sentiment analysis.
%
% Finally, \cite{Jha:2017} break ``sexism'' down into benevolent and hostile sexism. They apply the ambivalent sexism theory as proposed by \cite{Glick:1996}. The ambivalent sexism theory suggests that there are two forms of sexism, benevolent sexism, which on a surface level speaks positively on women, but on a deeper level seeks to assert their inferiority, and hostile sexism, which expresses a strictly negative point of view on women. The following examples illustrate benevolent and hostile sexism respectively: ``Women are like flowers who need to be cherished.'' and ``Jus gonna say it..again..DUMB BITCH! \#MKR''.\vspace{5mm}
%
% As online platforms seek to remove abuse occurring on their platform, data sets that have been gathered and annotated may have the abusive documents removed, thus requiring several rounds of re-annotation of abusive language. In an attempt to deal with this, we will experiment with using documents that are assumed have a higher chance of being abusive as they are posted in forums that are known to abusive. Using these documents we will seek to build different forms of embeddings and evaluating on previously annotated data. Further, in an attempt to mitigate annotator needs, we will build an abusive language potential system which utilizes supervised methods for Named Entity Recognition (NER), Gender Identification \citep{Sap:2014}, and sentiment analysis amongst other methods. The goal of this is to identify the probability that a document has potential to contain abusive content, in efforts to exact greater control over what documents an annotator is faced with. Additionally, such a system will allow us to identify documents which are clearly abusive. Thus, we will be able to provide an automated method to create a seed set of positive documents for abusive language detection.
\section{Abusive Language Detection}

% Introduce the field of abusive language detection
In recent years, the computational study of online abuse has seen a rapid increase in the number of papers dedicated starting with a handful of papers prior to $2016$ to a thriving research field with numerous papers, shared tasks, and workshops \citep{Vidgen:2020}. In spite of the growth in research dedicated to the detection of online abuse, the research field is still in its infancy with a number of open questions, including questions around definition of the task, annotation guidelines, and modelling techniques.
The earliest work in the field sought to address questions of cyber-bullying \citep{Chen:2012,Cho:2013,Reynolds:2011} and profanity \citep{Sood:profanity:2012,Sood:2013} with sparing focus on demographically specified abuse, such as racism, sexism, and anti-Semitism \citep{Warner:2012}. More recently, work on demographically specified abuse has surfaced as an independent task \citep{Tulkens:2015,Waseem:2016,Waseem-Hovy:2016,Park:2017,Samghabadi:2017,Karan:2018,Gorrell:2018,Stoop:2019,Meyer:2019,Palmer:2020,Vidgen:2020}. As a consequence of increased visibility of hate speech and abuse on online platforms, the academic inquiry into the computational detection has grown along with the regulatory responses \citep{Regulatory stuff: NetzDG, EUcommision on hate speech}.

Early, and contemporary computational work, has seen a great deal of focus to central questions around the task: how do we annotate and create datasets \citep{Waseem-Hovy:2016,Waseem:2017,Vidgen:2020} and understanding annotator interaction and performance \citep{Ross:2016,Waseem:2016,Vidgen:dynabench:2021}.
Early work focused on questions of marginalisation and oppression, for instance through the work of \citet{Waseem-Hovy:2016} who base their annotation on works in gender studies and critical race theory, and collect data based on gendered and racialised abuse; more recently data collection and annotation processes have moved towards a demographically blind process. Such early work was inspired by the marginalisation of certain bodies and the desire to develop computational tools to protect marginalised people \citep{Warner:2012}.marginalisation of certain bodies and the desire to develop computational tools to protect marginalised people \citep{Warner:2012}.
More recent work has instead directed its focus to demographically blind approaches to data collection and annotation, succumbing to ``marginalisation-blind'' annotation processes and guidelines. Although processes that do not take marginalisation into account, but instead seek to treat every group equally provide an allure of fairness, they also encode dominant discourse on abuse with the subsequent result of the resistance to oppression and marginalisation being treated the same as marginalisation. In concert with the growing evidence of racially biased content moderation tools \citep{Waseem:2018,Davidson:2019}, demographically blind annotation criteria and data curation pose a threat to the goal of developing tools that aid in ensuring people from the right from persecution. One such example is presented by \citet{Salminen:2018} who develop a taxonomy that includes ``anti-white'' as a target of hate on par with anti-Black hate in spite of whiteness as a hegemonic entity that marginalises \citep{McIntosh:1988}.
A result of this are egregious annotation choice, such as ``The  white  will  always  steal;  FUCK  YOU  TO  ALL  WHITES  RACIST'' labelled as hate speech \citep{Salminen:2018}, in spite of the comment speaking to ongoing racism and the historical exploitation enacted by white societies (e.g. the theft of cultural artefacts from colonised territories \citep{Frost:2019}, the numerous genocides committed by imperialistic colonial states \citep{Weisbord:2003}, and the theft of bodies in the transatlantic slave trade). Moreover, and perhaps of even greater concern, the annotation and curation processes of \citet{Salminen:2018} result in data responding to the abuse of authority committed by police as hate, in one such example they identify the following comment as hate ``did to that poor guy. 10 s of pepper spray directly into the face, run over foot etc. equal it up a little bit, except for the detail of having a fucking stroke. So it still wouldnâ€™t be exactly what the guy went through. Fucking discusting. They get a hard on power tripping others. They are just fucking cowards'', in all likelihood due to the aggressive nature of the comment.

Through such demographically uninformed processes of curating and making data, a danger of erasure of past and ongoing marginalisation and responses to it as well as critical responses to the violent abuse of authority as ``hate speech'' that should be subject to content moderation. The question of automated hate speech detection thus, for works such as \citet{Salminen:2018} is no longer ensuring the right to not be persecuted but instead insuring that processes of marginalisation remain unchallenged.
For these reasons, I use the datasets released in early work, specifically I use the \textit{Offence} dataset \citep{Davidson:2017}, \textit{Toxicity} dataset \citep{Wulczyn:2017}, and \textit{Hate Speech} dataset \citep{Waseem-Hovy:2016} in all computational chapters. For \autoref{chap:liwc} which examines the influence and generalisability of vocabulary manipulation, I also use the \textit{Hate Expert} \citep{Waseem:2016} and the \textit{StormFront} \citep{Garcia:2019} datasets. Each of these datasets share the common attributes that they are collected either from spaces that are hateful towards marginalised groups or have considerations of marginalisation encoded into the annotation guidelines. In \autoref{chap:mtl} I also use three datasets that are labelled for abuse but instead to tasks that are seemingly related. First, I use the \textit{Argument Base} dataset \citep{Oraby_fact_feel:2017}, the second dataset (\textit{Sarcasm}) is developed for sarcasm detection \citep{Oraby_sarcasm:2017}, and the final dataset, \textit{Moral}, examines the moral sentiments
In an early effort to address issues of annotator biases and under-sampling of some forms of data in the data curation process, \citet{Waseem:2017} propose a typology of abuse that aims to categorise abuse by how it is characterised rather than determining the exact form of abuse.
To this end, \citet{Waseem:2017} present a 2-dimensional typology of hate; the first dimension operates along implicit and explicitly expressed hate. Implicitly communicated hate, \citet{Waseem:2017} argue is hate that is communicated through subversive means by using code words and communicating implicit biases. Explicit abuse on the other hand is explicit in its intention to abuse, e.g. through the use of slurs. The second dimension concerns itself with the target of abuse that can either be a generalized other, or a specific group, the former category detailing abuse that is targeted towards small groups and individuals while the latter is aimed at generalised targets, e.g. larger demographics.
It's important to note that content may be simultaneously explicit and implicit, directed and generalised \citep{Waseem:2017}. For instance, content that implicitly targets Muslims, may simultaneously explicitly target a specific group of women.

\begin{table*}[ht]
\centering
\begin{tabular}{p{\textwidth/30}|p{0.45\textwidth}|p{0.45\textwidth}}
  & \textit{Explicit}    & \textit{Implicit} \\\hline
    \multirow{4}{*}{\rotatebox[origin=c]{90}{\textit{Directed}}}    &   {\scriptsize``Go kill yourself'',  ``You're a sad little f*ck'' \citep{Hee:2015a}}, \newline {\scriptsize ``@User shut yo beaner ass up sp*c and hop your f*ggot ass back across the border little n*gga''  \citep{Davidson:2017}}, \newline {\scriptsize ``Youre one of the ugliest b*tches Ive ever fucking seen'' \citep{Kontostathis:2013}}. & {\scriptsize ``Hey Brendan, you look gorgeous today. What beauty salon did you visit?'' \citep{dinakar2012common}, \newline ``(((@User))) and what is your job?  Writing cuck articles and slurping Google balls?  \#Dumbgoogles'' \citep{Hine:2016},\newline  ``you're intelligence is so breathtaking!!!!!!'' \citep{dinakar2011modeling}}\\\hline
  \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{Generalized}}} & {\scriptsize``I am surprised they reported on this crap who cares about another dead n*gger?'', ``300 missiles are cool! Love to see um launched into Tel Aviv! Kill all the g*ys there!'' \citep{Nobata:2016}, \newline ``So an 11 year old n*gger girl killed herself over my tweets? \^ \_ \^\ thats another n*gger off the streets!!'' \citep{Kwok:2013}}. & {\scriptsize``Totally fed up with the way this country has turned into a haven for terrorists. Send them all back home.'' \citep{burnap2015cyber}, \newline ``Gas the skypes'' \citep{magu2017detecting}, \newline ``most of them come north and are good at just mowing lawns'' \citep{dinakar2011modeling}} \\
\end{tabular}
  \caption{Typology of abusive language presented by \citep{Waseem:2017}.}
\label{tab:typology}
\end{table*}

Modelling for automated hate speech detection has also undergone a development from early to contemporary work. Early work was primarily focused on feature-based modelling \citep[e.g.][]{Waseem-Hovy:2016,Waseem:2016,Davidson:2017,Sahlgren:2018} whereas subsequent work has directed a greater attention to neural network based approaches \citep[e.g.][]{Kolhatkar:2021,Waseem:2018,Gamback:2017,Badjatiya:2017}. In this dissertation I follow a similar pattern of developing baseline models from feature-based models and suggest neural network architectures as extensions and improvements on these. In early work, Logistic Regression (LR) and Support Vector Machines (SVM) were the most frequently used models. As the scholarship has developed, specific types of neural networks have come to dominate the modelling, namely Convolutional Neural Networks and Long-Short Term Memory networks. In each chapter, I perform the review of the models that are pertinent to the work in the chapter. Here instead I provide a theoretical overview of the models, their components (e.g. dropout and activation functions) and their intended functionalities (i.e. the kind of data that they are designed to operate on and which assumptions are built into the model architectures).

\subsection{Datasets}\label{sec:datasets}

Here I provide an overview of the different datasets that are used throughout this dissertation. For each dataset, I introduce the curation rationale, the source of the datasets, the annotation guidelines, annotator selection, and finally how each of these dimensions influence the resulting dataset. In \autoref{sec:abuse_data} I describe the datasets annotated for hate speech and abuse and then in \autoref{sec:mtl_data}, I turn to the datasets used for the auxiliary tasks for \autoref{chap:mtl}.

\subsubsection{Hate speech and abuse datasets}\label{sub:abuse_data}

\paragraph*{Hate Speech} Published as the first publicly available dataset for hate speech and abusive language detection, \citet{Waseem-Hovy:2016} developed a dataset for detecting abuse towards gendered and racialised minorities. In an interview in the Let's Chat Ethics Podcast, Zeerak Waseem shared that the initial motivation for developing the dataset was the somewhat na{\"i}ve hope to address online harassment as experienced by women during \#GamerGate, a harassment campaign against female game developers and games journalists \citep{Massanari:2015}. This aim of developing tools that can protect marginalised people is apparent in the data sampling and the source of data. As a large amount of the GamerGate abuse occurred on Twitter, \citet{Waseem-Hovy:2016} use Twitter as a source of their data, collecting $16,914$ tweets labelled as ``sexist'', ``racist'', and ``neither''.
In efforts to ensure that their collected and annotated sample contains gendered and racialised abuse, they bootstrap their corpus collection by first search for common slurs against women, ethnic minorities, religious minorities, and sexual minorities to identify the salient terms and users for scraping.
To annotate this dataset, with the target group in mind, \citet{Waseem-Hovy:2016} develop $11$ questions to test whether a comment is hateful or not.
This set of questions focuses on breadth in the types of hate expressed rather than depth in each type. This is apparent as the tests ranges from asking about explicit forms of hate, such as the use of slurs to implicit forms like questions around stereotyping and the use of straw man arguments in criticisms of minorities.
\citet{Waseem-Hovy:2016} annotate their dataset and have their annotations verified by an external annotator.
Collectively, these decisions are made to ensure that there was a diversity in the forms of hate in addition to the sources. However, as they note, the racist abuse only comes from $9$ different accounts. Moreover, as salient terms were sampled for annotation, some terms (i.e. the hashtag for the Australian TV show My Kitchen Rules) are over-represented in the data. In spite of these issues, the annotations in this dataset are \textit{embodied} within the context of critical race and gender studies perspectives on abuse.

\paragraph*{Hate Expert} In an extension of the dataset proposed by \citet{Waseem-Hovy:2016}, \citet{Waseem:2016} sample $6,909$ tweets from the original scrape and have it annotated by two groups, in efforts to understand the influence of annotator biases. The first group consisted of ``feminist and anti-racism activists'' \citep{Waseem:2016} who annotate the sample of the dataset with one of four labels ``racist'', ``sexist'', ``both'', and ``neither''. The second group of annotators were recruited from CrowdFlower to re-annotate the sample annotated by the first group.\footnote{CrowdFlower has since been renamed Appen.}
The label set was expanded by \citet{Waseem:2016} to include the category ``both'', in acknowledgement that marginalisation can be expressed across multiple dimensions, in an \textit{Intersectional} manner. Comparing models trained on each group of annotators, \citet{Waseem:2016} find that models that use the annotations of the first group consistently outperform models trained on the second. \citet{Waseem:2016} argue that the reason for this difference is that models trained on the former group benefit from similarities in the understanding of hate speech, whereas the distinct subjective positionalities of the latter group, that does not have a salient unifying characteristic beyond their work on a crowd-sourcing platform, produces internally inconsistencies in labelling that render it harder for models to consistently identify patterns that they can learn from.
In presenting this dataset, \citet{Waseem:2016} propose that ideologues can take similar positions on a topic, given their subjective positionalities. They argue that only through a principled understanding of hate speech is it possible to annotate reliably for hate speech and that crowd-sourced annotations for hate speech display inconsistencies that to some degree erases the meaning of the term.
In using this dataset for this dissertation, I use the annotations provided by the feminist and anti-racist activists.

\paragraph*{Offence} Departing from the question of forms of hate speech and gender studies and critical race theory based annotation guidelines, \citet{Davidson:2017} turn instead to ask where the distinction between simply ``offensive'' speech and ``hate speech''.
Using a list of terms from Hatebase\footnote{Hatebase.org is a website that crowd-sources slurs and insulting turns of phrase. Due to marginalised people being disproportionately targeted, there is a distributional skew towards terms that target marginalised people in the number of terms.} to identify $33,458$ users whose tweets they sample.
From these users, they randomly sample $25,000$ tweets for annotation by CrowdFlower workers, resulting in $24,802$ annotated tweets. The crowd-workers were given guidelines to aid them in distinguishing between ``offensive content'', ``hateful content'' or ``neither offensive or hateful'', selecting only one for each tweet. \citet{Davidson:2017} instruct their annotators that hate speech is speech that ``is used in reference to certain groups that expresses hatred towards the group or is intended to be derogatory, to humiliate, or to insult the members of the group''. Moreover, they provide examples of such content which includes the straightforward ``Need to send these w******s back to their country''\footnote{Censoring of the slur is mine.} and the more conflicted ``I hate white trash''. The conflict in the latter stems from it being unclear whether the emphasis is on class-based hate or if it is targeting white people. While the former is less contentious, the latter would imply that white people too are targets of marginalisation on the basis of their race. However, as numerous scholars have argued, whiteness is the hegemonic force that marginalises \citep{CRT scholars}.
``Offensive'' content is provided as an alternative, less serious form of potentially unwanted content. This group is defined in contrast to the hateful class ``[o]ffensive content might use some of the same words we associate with hate speech but do not \textit{necessarily} constitute hate speech because the words are not used in the same context as ``hate speech````.\footnote{Emphasis added.}
From this definition it's clear that in spite of the instruction to select only one category, \citet{Davidson:2017} acknowledge that there is a potential overlap between offensive language and hate speech. 
Moreover, unlike the annotation guidelines proposed by \citet{Waseem-Hovy:2016}, the definition of offensive draws in the question of context. Illustrating this point, \citet{Davidson:2017} provide the following example ``Oh shush you know I love you f****t''. This use of context, provides space for inoffensive uses of slurs and insulting terms e.g. for reclaimed and in-group uses, a space that the annotation guidelines of \citet{Waseem-Hovy:2016} does not afford.
With the annotators being selected from CrowdFlower, the issues of multiple distinct ideologues in the annotator pool raised by \citet{Waseem:2016} are likely also manifest in this dataset.
However, as the dataset offers a space within which one can utter offensive but not hateful messages, it also offers the space to live spaces that dominant discourse on acceptability of language use would deem as unacceptable.\footnote{By dominant discourses on acceptability, I refer to what mainstream discourses deem as acceptable and unacceptable manners of speaking. However, such a discourses are internally inconsistent, as \citet{Oliva:2020} show, acceptable speech can come to include neo-nazi and white supremacist speech that threaten social cohesion while deeming speech by queer communities as toxic and inherently holding greater threat to the boundaries within which society should operate.}
In consideration of the marginalisation of queer people and people of colour, this dataset thus offers space for their uninterrupted existence. However, as the dataset is labelled for a multi-class classification problem, where a single label is assigned to each document, the dataset does not afford space to be exposed to hate speech in such contested spaces.

\paragraph*{Toxicity} Starting from a similar point as \citet{Davidson:2017}, \citet{Wulczyn:2017} develop a dataset  of $115,737$ comments to understand which types of conversations are likely to make users depart from the conversation. Departing from the early tradition of using Twitter as a source of data, \citet{Wulczyn:2017} consider the Wikipedia editor discussion pages. Taking a narrow view of behaviours that inhibits participation in conversations, \citet{Wulczyn:2017} focus on personal attacks and harassment, specifically asking their annotators whether which entity (the participant or a third party) is the subject of the attack. As a last positive category, they include whether it is ``[a]nother kind of attack or harassment'', thus relegating all forms of harassment that are not directed at specific individuals to a residual category. The dataset thus is comprised of ``personal attacks'' and ``other forms of harassment''. As the study is specifically grounded in identifying personal attacks, this categorisation of various forms of personal attacks and a residual category as positive instances is in line with the aims of the data, if not the description.
Using this definition, \citet{Wulczyn:2017} select a random sample of $37,611$ and have it refereed by $10$ annotators for personal attacks, resulting in only $0.9\%$ of the labelled data being assigned the positive label. Subsequently, they identify $78,126$ comments that had been made by users whose comments were moderated from the discussion pages, for each user taking $5$ comments that they made around the moderated comment, and similarly subject them to annotation this time resulting in the positive class occupying $16.9\%$ of the labelled data.
Similarly to \citet{Waseem:2016} and \citet{Davidson:2017}, \citet{Wulczyn:2017} use CrowdFlower to obtain their annotations and subsequently are prone to similar issues in their data. However, to curb such issues they obtain $10$ annotations for each comment, allowing to compute a majority vote that takes a broader perspective on the comment into account. In spite of this approach, where those annotators are from and what their position on personal attacks are, and their ability to identify subtle attacks, still remain uncertain resulting in a dataset that may take a global position or a culturally grounded position on identifying personal attacks, e.g. if a large subset of annotators live in India, a subset of the data may very well reflect Indian perceptions of personal attacks.
The resulting dataset has been constructed to understand which comments are likely to turn discussions ``toxic'' as a result of personal attacks. Through the use of $10$ annotators for each comment, \citet{Wulczyn:2017} aim for a global understanding of toxicity derived, in part, from personal attacks.
Similarly to \citet{Davidson:2017}, there appear to be no consideration of the experiences of abuse against marginalised communities.
Considering Wikipedia's well documented issues with being a hostile space to women \citep{Torres:2016} and the distribution of gender crowd-workers often veering towards a greater representation of men than women \citep{Posch:2018}, the lack of such a consideration may further entrench subjective positions that are hostile towards women into the datasets and subsequently into the models.

\paragraph*{StormFront} Focusing on the white supremacist web-forum StormFront, \citet{Garcia:2019} collect a dataset of $10,568$ sentences annotated by three of the authors for containing hateful utterances. Similarly to \citet{Davidson:2017} and \citet{Wulczyn:2016}, \citet{Garcia:2019} employ a marginalisation-blind definition and understanding of hate speech. In the case of a white supremacist web-forum, employing a marginalisation-blind definition is unlikely to be challenged as the participants are unlikely to engage in derogation against white, straight, cisgender men.
The decision for using StormFront as a source of data was motivated by the prevalence of ``pseudo-rational discussions of race''.
Moreover, this dataset further distinguishes itself from prior datasets by annotating on a sentence level.
The authors argue that annotating on a sentence level can reduce the confounding factors by only addressing content which is explicitly hateful.
While this may, in some instances have little effect as the surrounding sentences bear no impact on whether a sentence is hateful. This particularly holds for explicit hate speech.
However, for subtle forms of hate speech, conducting sentence level annotation may obscure hate that is only apparent when considering a post in its entirety rather than its sentence level components. In order to address this issue, \citet{Garcia:2019} introduce a ``related'' tag which is to be used when individual sentences do not convey hate but the combination of several sentences in sequence do convey hate.
This method for mitigation does not account for longer sequences of sentences that convey hate, as is often the case for subtle forms of hate speech and dog whistles.
Moreover, as \citet{Garcia:2019} take a very conservative position on what constitutes hate, for instance, the use of a derogatory term, on a white supremacist web-forum, ``cannot be said to be a deliberate attack, taken without any more context, despite it likely being offensive.'' For this reason, \citet{Garcia:2019} argue that simply the occurrence of slurs weaponised against marginalised communities cannot be said to be hateful.
Thus, while initially side-stepping the issue of marginalisation-blind definitions by sourcing data from a white supremacist web-forum, it is softly reintroduced by taking a conservative stance on what constitutes hate.

\subsubsection{Non-abuse datasets}\label{sec:mtl_data}

\paragraph*{Sarcasm} \citet{Oraby_sarcasm:2016} develop a dataset for sarcasm detection in dialogues. The dataset was developed in order to address the lack annotation for subtypes of sarcasm, i.e. rhetorical questions and hyperbole, at scale in previous datasets.
Sourcing their data from the Internet Argument Corpus (IAC) \citep{Abbott:2016}, \citet{Oraby_sarcasm:2016} annotate their data for ``generic sarcasm, rhetorical questions, and hyperbole''.
In order to generate a dataset from the IAC, \citet{Oraby_sarcasm:2016} train a ``weakly-supervised pattern learner'' \citep{Oraby_sarcasm:2016} to identify a set of $30,000$ posts, filtering two thirds of the posts that don't contain any ``not-sarcastic'' cues and annotate the remaining $11,040$ posts in quote-response pairs for annotation on Amazon Mechanical Turk.
Similarly to the abusive language datasets annotated on CrowdFlower, this choice of annotators can introduce biases stemming from the subjective embodiments of the human annotators and the geo-cultural contexts in which they exist.
Following the annotation process a dataset of $6,520$ posts (with a $50\%$ split of sarcastic and not-sarcastic posts) is obtaned and released.
Examining the dataset for suitability for machine learning experiments, \citet{Oraby_sarcasm:2016} train a linear SVM with Stochastic Gradient Descent (SGD) training and L2 regularisation obtaining F1-score of $0.74$ using features derived from Word2Vec \citep{Mikolov:2013}.

\paragraph*{Argument Basis} Investigating the characteristics of factual and emotional argumentation styles, \citet{Oraby_fact_feel:2015} also draw on the IAC as the source of data. Considering quote-response pairs, each response is annotated for whether the argument presented in the response based primarily in fact or feeling.
\citet{Oraby_fact_feel:2015} present $10,003$ from the IAC for annotation by $5-7$ crowd-workers on Amazon Mechanical Turk for annotation selecting a value ranging from $-5$ to $5$ to indicate whether the response is a feeling or fact-based argument, where negative values indicate that the argument basis is dominated by an emotional argumentation style and positive values indicate a fact-based argument.
Each document is then given a binary label indicating its argument basis, where all texts with a score greater than $1$ are assigned as fact-based, all texts with a score lower than $-1$ are assigned to the feeling-based class, and all scores $[-1, 1]$ are discarded.
This annotation process results in $3,466$ fact-based and $2,382$ feeling-based documents.
Similarly to the previously examined datasets that utilise crowd-workers, this dataset is also subject to the contexts which the individual annotators exist within. For instance if an annotator is from a culture where feeling-based argumentation is not experienced as impassioned but instead supportive of facts, they may be likely to rate some documents as more fact-based than annotators who hail from cultures that emphasise fact-based argumentation would deem as relying on an emotional argumentation style.
The subjectivity of the annotation task may provide an explanation for why $4,155$ or more than $41\%$ of the documents are discarded due to being rated, in aggregate, as dominated by neither fact or emotion.

\paragraph*{Moral Sentiment} The final dataset used in this dissertation is the Moral Foundations Twitter Corpus \citep{Hoover:2019}. This dataset provides $35,108$ tweets annotated for $10$ different categories of moral sentiment, introducing the task of moral sentiment prediction.
A task, and dataset designed to allow psychology researchers to investigate the relationship between comments made around events and the moral foundations found in such comments made on Twitter.
% This task was introduced to explore the utility of NLP modelling techniques for psychology research, namely measuring psychologically relevant psychological constructs in tweets.
\citet{Hoover:2019} draw from research in psychology around human morality using a five-factor taxonomy that reveals insights into the moral foundations \citep{Graham:2009,Graham:2013} that underlie comments about and attitudes towards topics.
Each of the five factors are represented through a binary, where one end of the binary represents a virtue and is contrarian to the other, representing a vice.
\citet{Hoover:2019} argue that the human expression of vice and virtue are distinguishable from one another through distinct language use for each.
The five factors introduced are \texttt{care}, ``concerns related to caring for others'' and \texttt{harm}, ``concerns related to not harming others''; fairness, ``concerns related to fairness and equality'' and \texttt{cheating}, ``concerns related not not cheating or exploiting others''; \texttt{loyalty}, ``concerns related to prioritising one's ingroup'' and \texttt{betrayal}, ``concerns related to not betraying or abandoning one's ingroup''; \texttt{authority}, ``concerns related to submitting to authority and tradition'' and \texttt{subversion}, ``concerns related to not subverting authority or tradition''; \texttt{purity}, ``concerns related to maintaining the purity of sacred entities, such as the body or a relic'' and \texttt{degradation}, ``concerns focused on the contamination of such [sacred] entities.''
Noting that there is low occurrence of moral sentiments expressed in a random sample of tweets, \citet{Hoover:2019} collect tweets related seven different discourse domains where the occurrence of moral sentiment is likely to at a high rate: Black Lives Matter, All Lives Matter, Baltimore protests following the death of Freddie Gray, the 2016 presidential elections in the United States of America, hurricane Sandy, the \#MeToo movement, and offensive language, re-annotating a sample of \citet{Davidson:2017} for the moral sentiments.
For annotation, \citet{Hoover:2019} train $8$ undergraduate research assistants to an expert-level familiarity with the moral foundations taxonomy, annotating $4,000 - 6,000$ for each discourse domain. The annotators are trained through a training sessions and, in early stages, discussion surrounding annotator disagreement.  
The annotator selection procedure here thus develops on the suggestion of \citet{Waseem:2016} to use expert annotators to describing a means of training expert annotators for a highly subjective task. Interestingly, as the annotation process continues past early stages, annotator disagreements are not resolved, instead the authors opt for expressing the inherent subjectivity of the human annotation task.

\subsubsection{Non-English datasets for abuse}
\zw{See hatespeechdatasets.com and ACL anthology}

In this dissertation, I focus my attention to detecting abuse in English language datasets as my methods do not map to other languages. However, an important growing body of research and resources are being developed for other languages such as Arabic \citep{Arabic abuse papers}, Chinese \citep{Chinese abuse papers}, Croatian \citep{Croatian papers}, Danish \citep{Danish abuse data}, French \citep{French papers}, German \citep{German papers}, Greek \citep{Greek papers}, Indonesian \citep{Indonesian papers}, Italian \citep{Italian papers}, Polish \citep{Polish papers}, Portuguese \citep{Portuguese papers}, Slovene \citep{Slovenian papers}, Spanish \citep{Spanish papers}, Turkish \citep{Turkish papers}, and Urdu \citep{Urdu papers}. Beyond these mono-lingual resources and approaches, there is also a body of work dedicated to abuse in code-switching contexts \citep{Code switching papers}.

Developing models for each individual language, and in particular resources that address abuse that code-switches, require an attention to the particularities of the different languages and cultures, just as model development for English requires researchers to be attuned to the particularities and cultures represented in English language use.


\subsection{Generalisable Machine Learning Models for Abusive Language Detection}
A common criticism of many current computational methods for abuse detection is that they have poor generalisability onto other datasets. Although this issue of non-generalisability poses a serious issue for the abuse community, it has received relatively little attention \citep{Waseem:2016,Waseem:2018,Karan:2018,Wiegand:2019,Swamy:2019,Fortuna:2021,Glavas:2020} in comparison to single-dataset classifier performance. In each of the computational chapters (see \cref{chap:liwc,chap:mtl}), I also provide consideration of how well the trained models perform on out-of-domain datasets.
In the pursuit of models that generalise well onto other datasets, researchers have proposed a variety of architectures. As an initial investigation into the question of generalisability, \citet{Waseem:2016} note that the best performing classifier on the dataset they propose does not generalise well onto the \textit{Hate Speech} dataset, noting that the performance of their classifier drops by more than a $25\%$.
Using Multi-Task Learning (MTL),\footnote{Multi-task learning allows for training models using multiple different datasets, for distinct machine learning tasks, where one (main) task is given higher priority and all other tasks are treated as auxiliary tasks.} \citet{Waseem:2018} address the issue of poor generalisability between the \textit{Hate Speech} and \textit{Hate Expert} (combined into a single dataset) and the \textit{Offence} dataset, showing that a MTL framework can be used for training models that can generalise onto from one cultural context onto another. Moreover, considering the results posted by \citet{Waseem:2018}, it appears that there is a trade-off between well-performing in-domain models and well-performing cross-domain models, where cross-domain improvements appear to come at the cost of in-domain performance, where out-of-domain performance is computed by mapping the classes in the in-domain datasets to the out-of-domain dataset.

\citet{Karan:2018} further explore the question of cross-dataset generalisability using a linear SVM model. \citet{Karan:2018} approach the task of out-of-dataset performance as a classical domain adaptation task and use the FEDA framework \citep{Doume:2007}, finding that without significant procedures for domain adaptation, there is poor generalisability. Similarly to \citet{Waseem:2018}, \citet{Karan:2018} find that cross-domain performance comes at the cost of in-domain performances but with large out-of-domain improvements. One difference between \citet{Karan:2018} and the previously described studies is that \citet{Karan:2018} reduce the learning task to a binary classification task of ``abusive'' and ``non-abusive'' documents.

The last approach to generalisation I consider is the work by \citet{Fortuna:2021}. In this paper, the authors compare four different models for out-of-domain classification: a Bag-of-Words SVM model, a Continuous Bag-of-Words FastText model, a BERT model \citep{Devlin:2019}, and an ALBERT model \cite{Lan:2020}. The latter two being transformer-based language-models that are fined-tuned to the task of predicting abuse.
\citet{Fortuna:2021} propose a different class organisation to past studies, first they propose as generalised class organisation that collapse classes across datasets into a smaller, generalised subset that maps across datasets. For instance, the ``sexist'' class provided by \citet{Waseem-Hovy:2016} and the ``misogyny'' class provided by \citet{Fersini:2018} into a ``misogyny-sexism'' class. Each of the generalised classes are binarised to allow models trained with other standardised labels to predict on them.
Using these generalised classes, \citet{Fortuna:2021} show that by using methods that capture more complex word-interactions, out-of-domain performance generally improves within and out of domain, subject to the classification task.
Specifically, they find that when classes have significant overlaps across datasets in their rationalisation of what the are to represent then models trained on those classes will map well onto the rest.
Conversely, when the classes have a little overlap, the models will generalise poorly onto the new dataset.
Moreover, \citet{Fortuna:2021} identify that some dataset combinations produce poor generalisation between each other regardless of the models used.
This, in concert with their conclusion that dataset overlap and out-of-domain similarities are drivers of model generalisation has two implications.
First, current computational models can, to some degree, adapt onto new distributions and samples but models using words as input are poorly suited for learning general trends of a wide variety of abuse, including closely related concepts such as ``toxicity'' and ``severe toxicity'' \citep{Fortuna:2021}.
Second, as models do not generalise onto other concepts, even if closely related, research in the detection of online abuse must either develop methods that can generalise onto studying different objects and perspectives of online abuse, or datasets must be annotated following highly similar annotation guidelines at the cost of the depth and breadth of concepts that can be explored.

\section{Models}\label{sec:model_background}
% TODO Introduce modelling
% TODO Introduce LIWC dictionary in depth
% TODO Introduce Linear models: SVM, LR
% TODO Neural Networks: Single-task + MTL
% TODO LSTM (and recurrence)
% TODO MLP
% TODO CNN and pooling
% TODO Introduce vectorisation & tensorisation
% TODO Introduce byte-pair encoding
% TODO Introduce Dropout
% TODO Introduce optimisaton:
% TODO L1, L2, and elasticnet
% TODO Softmax, Losses (NLLL), Optimisers (SGD, ADAM, ADAMW, ASGD)
% TODO Hyper parameter tuning
% TODO Metrics: precision, recall, F1, accuracy
% TODO Fairness work
% TODO Limitations

In this section I provide an introduction to the different modelling techniques that I use throughout the dissertation.

\subsection{Data Encoding}

In order for models to read the data, it is necessary to provide the models with machine readable representations of the data. The first step to creating such machine readable representations is to provide each unique token with a numerical index.
The numerical index, and what it represents is a matter of how the data is pre-processed. For instance, in \cref{chap:liwc}, I represent tokens in three different ways. 
First, I represent tokens using their surface form, that is each word is represented in its entirety following a tokenisation process where all words are lower-cased and punctuation markers are split from the word (see \cref{chap:liwc} for further pre-processing steps). Second, I take the surface forms of tokens computed and represent them as the categories of the Linguistic Inquiry and Word Count (LIWC) categories each token induces (see \cref{chap:liwc} for further detail). Finally, in \cref{chap:liwc,chap:mtl} I represent tokens as the subwords that they consist of. 
In this section, the sub-word forms while omitting the surface-token and LIWC-token forms as these rely on simple pre-processing and mapping steps that are described in more detail in \cref{chap:liwc}.

\subsubsection{Byte-Pair Encodings}

Byte-Pair Encodings were introduced to the NLP comunity by \cite{Sennrich:2016} for the task of Neural Machine Translation to address the issue of out-of-vocabulary tokens. In this paper, the authors argue that for word-level machine translation there is not always a one-to-one relationship between a word in the source language and its translation into a target language. \citet{Sennrich:2016} illustrate this point through compound words, where a compound word represents a specific entity that is represented through multiple words in the source language, e.g. the German \textit{Abwasser \textbf{|} behandlungs \textbf{|} anlange} and its English translation \textit{sewage water treatment plant} \citep{Sennrich:2016}.
\citet{Sennrich:2016} propose to compute sub-words using the byte-pair encodings algorithm proposed by \citet{Gage:1994}. While the algorithm proposed by \citet{Gage:1994} operates on bytes and seeks to develop a new representation of bytes that can compress their representation, \citet{Sennrich:2016} seek to operate on the sub-units of words, that is a sequence of characters. In both cases, the algorithm operates by considering the input and identifying frequently occurring patterns that can be represented in terms of a single unit.
In efforts to obtain a sub-word representation, \citet{Sennrich:2016} initialise their algorithm initially with a vocabulary consisting of each unique character token in the dataset and then count all symbol pairs (e.g. character co-occurrences) and merge the most frequently occurring pairs and adding it to the vocabulary. This merging process is repeated a number of times, where the total number of merge operations is a hyper-parameter set by the designer of the sub-word representation. The size of the vocabulary following this process will be the size of the original vocabulary plus the number of merge operations that are set by the designers \citep{Sennrich:2016}
In terms of language, using sub-words to represent documents can minimise the number of out-of-vocabulary tokens in the validation and evaluation sets of a dataset, as the likelihood of a word not being represented decreases as it is broken down into its subwords.

In this dissertation, I use the Byte-Pair Embeddings developed by \citet{Heinzerling:2018}. These embeddings were trained for $275$ languages using the Wikipedia pages in each language as the source of data.

%\subsection{Annotation Guidelines}
%\zw{Write about annotator guidelines}
%
%\subsection{Annotator Selection}
% \zw{Write about annotator influences}
% There are some interesting discrepancies in the selection of annotators for the datasets that we apply our models to. \citet{Waseem:2016} select their annotators based on socio-political positions, controlling for a specific interpretation of abuse. On the other hand \citet{Wulczyn:2017} select their annotators from the users of the Wikipedia Talk pages. However, the Wikipedia editor community has been accused of being a highly male space that is unwelcoming to women \cite{CITE: Cite article talking about anti-women culture on wikipedia}. This suggests that the influence of their selection of annotators, who are culturally situated in the norms and culture of the Wikipedia editor community, are also likely to be less attuned to content that may be offensive to women, but is accepted communicative practices within the Wikipedia editor community.
%
% On the other hand \citet{Garcia:2019} and \citet{Davidson:2017} select annotators that are removed from the context of the documents they are annotating. This suggests that global understandings of what constitutes abuse are possible, and that it is possible to annotate without a deep understanding of the issues and communicative practices of the specific communities that are being investigated.
%
% While we accept that the influence of annotation guidelines have strong influences on the subject that is being examined (e.g. \citet{Davidson:2017} examine the differences in what is merely offensive and what is hateful, \citet{Garcia:2019} examine what is hateful from a white supremacist community and what is not, and \citet{Wulczyn:2017} examine things that make conversations toxic and hostile), we assume that these different guidelines and questions highlight different aspects of abuse. Through our efforts to develop methods that can identify different forms of abuse across different datasets, we accept the assumption that there are some global understandings of abuse that can be learned by machine learning models. We revisit this assumption in \autoref{chap:disembodied}.
%
% \citet{Waseem:2016} argue that due to different backgrounds and political stances, the operationalisation of annotation guidelines differ from person to person. They show that this then influences the quality of annotations of datasets, as a lack of control of socio-political backgrounds also suggests a lack of control of what is being annotated. They argue that such differences in operationalisation across a dataset influences what machine learning models are able to predict. Conversely, \citet{Founta:2017} operate with the assumption that with a large numbers of annotators, such differences will even out, such that dominant discourse understanding of what constitutes abuse and hate will even out. However, considering the argument that \citet{Douglas:1966} makes about what constitutes dirt is both contextual and cultural, what remains through majority voting by a large number of annotators, will be few salient discourses of what constitutes abuse and hate speech. Scaling annotations across cultural divides then also suggests that only what is universally accepted as abuse should be considered as abusive.
%
% This notion of dominant discourse, and culturally agnostic definitions of abuse stand in stark contrast to the widespread calls for moderation of abuse to be culturally contingent \cite{CITE: Articles about how content moderation fails}. More importantly, as \citet{Waseem:2017,Davidson:2019} and \citet{Sap:2019} argue, such dominant discourse perspectives on acceptability are likely to reproduce cultural, and often racial, biases towards marginalised communities.
%
%
%\subsection{Platform Affordances}
%
% \zw{Write theoretically about platform affordances, base in STS litt.}
% As the datasets differ quite significantly in the sizes of the raw number of documents as well as the vocabulary sizes. Moreover, as the datasets are selected from different websites with different communities, purposes, and means of interaction; the data sampled from each platform may differ in content as well as style. Considering for instance \citet{Waseem:2016}, this dataset was collected on Twitter while the maximum length of a tweet was $140$ characters. Documents are thus short as they are given an upper limit on the number of characters. On the other hand, \citet{Wulczyn:2017} collected their data from the Wikipedia Editor Talk pages, where comments are not limited by length. Additionally, these two domains differ in that conversations on Twitter may have no particular topic, conversations on Wikipedia Talk pages always refer back to a specific topic and the conversation of how to address a particular edit to a page. Finally, given Wikipedia's ongoing issues with recruiting editors from a diverse set of backgrounds \cite{CITE: Wikipedia editors issue} and Twitter's comparatively broad user base \cite{CITE: Twitter userbase by demographic ref} may influence which dialects are represented on the platforms, which patterns of speech (e.g. sociolects, slang, and shorthand) occur, and the style of the discussions and conversations.
%
%
\section{Models}\label{sec:model_background}
%
% Throughout this thesis, we develop and train several different model types to test our hypotheses. We use a mixture between linear and non-linear models both to act as points of comparison with one another and to test how well both simple and more complex models perform given our tasks. In each chapter, we will go into greater detail with regard to the implementation details for each model while we provide a theoretical description of each of these models here.
%
\subsection{Byte-Pair Encoding and Linguistic Inquiry and Word Count}

\zw{Add info on LIWC dictionary size}

\subsection{Dropout and Early Stopping}\label{sec:dropoutearly}

\subsection{Input Encoding}

\zw{Explain the vectorisors used.}
\zw{Explain onehot and embedding layers}
%
% As embedding layers are layers that are optimised, it is unnecessary for the researcher to perform feature selection \cite{CITE: Papers that say no feature selection is needed}, although it can be a benefit \cite{CITE: Papers that take feature selection into account}. The process of selecting features for a model to consider implores the researcher to have a firm grasp of the concepts they are seeking to examine. This required intentionality of the researcher also comes with the risk that the researcher may, intentionally or unintentionally, omit features that illuminate a pattern in the data that the machine learning model can take advantage of.  Here optimisable representations excel if features are not pre-selected for them, as through multiple rounds of updating the layer, the resulting representation takes advantage of the patterns that emerge from the data.
%
\subsection{Softmax, Loss, and optimisers}

\subsubsection{Loss}

\paragraph{Negative Log Likelihood}

\paragraph{Cross Entropy}

\paragraph{L1 and L2}

\subsection{Activation Functions and Attention Layers}

\subsubsection{Sigmoid}

\subsubsection{Tanh}

\subsubsection{Rectified Linear Unit}

\subsection{Logistic Regression}

\subsection{Support Vector Machines}

\subsection{MLP}

\subsection{LSTM}
% TODO Start by explaining an RNN
% TODO Explain how LSTMs differ from RNNs

\subsection{RNN}

\subsection{CNN}

\subsubsection{Max Pooling}

\subsection{Multitask Learning}
%
\zw{CITE: Add citations to the section below.}
The Multi-task learning framework was initially proposed by \citet{Caruana:1997} as a way to train a model for a specific primary task by leveraging that (multiple) auxiliary tasks may be related and could thus provide inductive biases for the resulting model to take advantage of for its primary task. The multi-task learning framework can be trained in two different ways, through hard parameter sharing or soft parameter sharing. Training Multi-task learning models using hard parameter sharing is performed by having (some) hidden layers in the model that are shared by all tasks. When training each task, the model updates all layers of that task, including the shared hidden layers. Notably, while hard parameter sharing rely on (some) shared layers, the output layers are not shared between tasks \cite{CITE: Hard parameter sharing paper}. On the other hand, models that are trained using soft parameter sharing do not share any layers, instead the parameters of each task are regularised to be similar \cite{CITE: Soft parameter sharing paper - See Sebastian Ruder's blogpost on MTL for reference}. In \autoref{fig:mtl_types} we see the two different types of models. As seen in \autoref{fig:mtl_hard}, while each task have individual input and output layers, they all share a hidden layer. In \autoref{fig:mtl_soft}, we see that each task has its own model that would be unrelated to one another, if not for the fact that the parameters of each layer are regularised to be similar to each other. As researchers are frequently interested in one task over others, one can assign a primary task and a number of auxiliary task through a range of strategies, including weighting each task and the probability with which each task is selected for training.

\begin{figure}
 \centering
 \begin{minipage}{0.5\linewidth}
   \centering
   \includegraphics[scale=0.75]{Figs/multitask_hard.jpg}
   \caption{Depiction of Multi-task learning framework using hard parameter sharing.}
   \label{fig:mtl_hard}
 \end{minipage}
 \begin{minipage}{0.5\linewidth}
   \centering
    \includegraphics[scale=0.75]{Figs/multitask_soft.jpg}
   \caption{Depiction of Multi-task learning framework using soft parameter sharing.}
   \label{fig:mtl_soft}
 \end{minipage}
 \caption{Parameter sharing strategies for Multi-task learning.}
 \label{fig:mtl_types}
\end{figure}

While the idea of inductive biases from related tasks provides a compelling argument for examining multi-task learning for abuse and hate speech detection, there are some interesting attributes to the framework. First, as multi-task learning is compatible with neural networks, researchers can forego feature selection similar to other neural network approaches, with the same benefits and risks as described previously.
Second, while the ensemble model training framework may appear very similar to multi-task learning framework, a key dissimilarity is apparent in that multi-task learning models seek to share information between the different tasks; for hard parameter sharing models this sharing occurs through shared layers \cite{CITE: Hard parameter sharing paper}, while for soft parameter sharing models this sharing happens through the regularisation that ensures similarity of the layers across the models for each task \cite{CITE: Soft parameter sharing paper}.
Third, for hard-parameter sharing models the complexity of developing the model is reduced as the input and output layers are task independent, while at least one layer of the remainder of the model may be shared. Thus, only a single model is trained, where the researchers need only to concern themselves with the layers that are not shared, rather than concern themselves with full models and how to balance them.
Fourth, as \cite{Caruana:1997} show, the framework allows for training for several distinct tasks while leveraging the similarities shared by each individual task.
For hard parameter sharing models, this approach also introduces the risk (and opportunity) of a single task dominating the representation of the model, due to either more data being available or a task being selected with for training with greater probability than the remaining tasks. \citet{CITE: Weighting paper} argue, this risk can be mitigated by either weighting the loss function, such that loss of each task is controlled by the researcher and the importance they wish to provide each task \cite{CITE: Weighting paper}. However, as \cite{CITE: Weighting/aux task paper} show, there is also an opportunity in this risk. By assigning the task of primary interest a higher weight than all other tasks, the model can be guided towards prioritising what is learned from this task over all others \cite{CITE: Paper with auxiliary tasks}. Selecting such weighting of the different tasks, in a similar vein to feature selection assumes that the researcher has knowledge and a hypothesis about how the different tasks are likely to influence each other.
Fifth, when working with different datasets for similar and distinct tasks alike, directly leveraging them outside of a multi-task learning model can be a cause for concern due to differences in collection rationales, data sources, or annotation strategies \cite{Waseem:2018}. However, through both weighting of the different tasks and the fact that each task has either its own input and output layers or its own model, such concerns can be alleviated due to either limited shared layers that are optimised or due to distinct models being trained that are regularised to minimise dissimilarity, depending on which parameter sharing strategy is used.
Finally, in the event that an auxiliary task does not contribute to the primary task as the researcher had hypothesised, it may still contribute to the overall generalisability of the model as the offending task will act as regulariser for the primary task, as it introduces noise into shared layer according to \cite{CITE: Cite paper that argues the regularising effect of aux tasks}.

\zw{Add paragraph on how MTL works with mini batching, batch selection, etc.}
%
%
\subsection{Bayesian Hyper Parameter Tuning}\label{sub:bho}

\subsection{Metrics}

\zw{INSERT: Explanation of F1, macro-F1, Precision, Recall, Accuracy}

\section{Fairness}\label{sec:fairlitt}

Previous work on bias and fairness in machine learning pipelines operate within four overarching logics:

\begin{enumerate}
  \item{A descriptive strand which aims to map out models and datasets with their intended uses and limitations, }
  \item{the quantification and analysis of disparities in model performances,}
  \item{the mitigation of biases that are present in the models and datasets, and}
  \item{imaginaries of more equitable futures for AI.}
\end{enumerate}

\subsection{Mapping Limitations}

% Mapping
\cite{Mitchell:2019}
\cite{Bender-Friedman:2018}
\cite{Hovy-Spruit:2016}
\cite{Blodget:2020}
\cite{Holstein:2019}

\subsection{Quantifying harms}
\cite{Buolamwini:2018}
\cite{Kulynych:2020}
\cite{Shah:2020}
\cite{Vanmassenhove:2018}
\cite{Waseem:2016}
\cite{Derzynski:2016}
\cite{Birhane:2020}

\subsection{Quantification}
\cite{Agarwal:2018}
\cite{Romanov:2019}
Note that \cite{Sap:2019} misuse \cite{Blodgett:2016} to provide assumed demographic affiliation of the author, however author attributes are computationally estimated and \cite{Blodgett:2016}'s method does not afford such attribution.
\cite{Davidson:2019}
\cite{Zhao:2017}

\subsection{New Futures}

\cite{Yimam-Biemann:2018}
\cite{Bingel:2018}
\cite{Kalluri:2019}

\section{Summary}
% In this chapter, we have introduced the NLP work related to thesis and sought to show how we will build and expand on this work.
